{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Takeaway name classifier deep learning live demo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai import *\n",
    "from fastai.text import *\n",
    "import string\n",
    "from unidecode import unidecode\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a Fastai Data Loader\n",
    "\n",
    "Load in the dataframe and extract indexes for training, validation and balanced trainings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('after_21_11_2019_meeting_clean.csv')\n",
    "\n",
    "valid_idx = df[df.valid].index\n",
    "train_idx = df[~df.valid].index\n",
    "\n",
    "bal_idx = []\n",
    "for k, v in zip(df.index, df.bal):\n",
    "    bal_idx += [k]*v\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LetterTokenizer(BaseTokenizer):\n",
    "    \"Character level tokenizer function.\"\n",
    "    def __init__(self, lang): pass\n",
    "    def tokenizer(self, t:str) -> List[str]:\n",
    "        out = []\n",
    "        i = 0\n",
    "        while i < len(t):\n",
    "            if t[i:].startswith(BOS):\n",
    "                out.append(BOS)\n",
    "                i += len(BOS)\n",
    "            else:\n",
    "                out.append(t[i])\n",
    "                i += 1\n",
    "        return out\n",
    "            \n",
    "    def add_special_cases(self, toks:Collection[str]): pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "itos = [UNK, BOS] + list(string.ascii_lowercase + \" -'@&)(.\" +\"0123456789\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab=Vocab(itos)\n",
    "tokenizer=Tokenizer(LetterTokenizer, pre_rules=[], post_rules=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df.iloc[train_idx, [0,3]]\n",
    "bal_df = df.iloc[bal_idx, [0,3]]\n",
    "valid_df = df.iloc[valid_idx, [0,3]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier with Just Eat data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = TextClasDataBunch.from_df(path='.', train_df=bal_df, valid_df=valid_df,\n",
    "                         tokenizer=tokenizer, vocab=vocab,\n",
    "                         mark_fields=False, bs=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = text_classifier_learner(data, AWD_LSTM, drop_mult=0.4, bptt=70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNNLearner(data=TextClasDataBunch;\n",
       "\n",
       "Train: LabelList (50000 items)\n",
       "x: TextList\n",
       "xxbos   xxunk f a l a f i l,xxbos   xxunk f a l a f i l,xxbos   xxunk f a l a f i l,xxbos   xxunk f a l a f i l,xxbos   xxunk f a l a f i l\n",
       "y: CategoryList\n",
       "Other,Other,Other,Other,Other\n",
       "Path: .;\n",
       "\n",
       "Valid: LabelList (5000 items)\n",
       "x: TextList\n",
       "xxbos   2 4 7,xxbos   xxunk d e s s e r t s,xxbos   1 0   t o   1 0   i n   d e l h i,xxbos   1 3   0 2   d e s s e r t   c a f e,xxbos   1 4 2 3   c h i n a   k i t c h e n\n",
       "y: CategoryList\n",
       "Burgers,Desserts,Indian,Desserts,Chinese\n",
       "Path: .;\n",
       "\n",
       "Test: None, model=SequentialRNN(\n",
       "  (0): MultiBatchEncoder(\n",
       "    (module): AWD_LSTM(\n",
       "      (encoder): Embedding(46, 400, padding_idx=1)\n",
       "      (encoder_dp): EmbeddingDropout(\n",
       "        (emb): Embedding(46, 400, padding_idx=1)\n",
       "      )\n",
       "      (rnns): ModuleList(\n",
       "        (0): WeightDropout(\n",
       "          (module): LSTM(400, 1150, batch_first=True)\n",
       "        )\n",
       "        (1): WeightDropout(\n",
       "          (module): LSTM(1150, 1150, batch_first=True)\n",
       "        )\n",
       "        (2): WeightDropout(\n",
       "          (module): LSTM(1150, 400, batch_first=True)\n",
       "        )\n",
       "      )\n",
       "      (input_dp): RNNDropout()\n",
       "      (hidden_dps): ModuleList(\n",
       "        (0): RNNDropout()\n",
       "        (1): RNNDropout()\n",
       "        (2): RNNDropout()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1): PoolingLinearClassifier(\n",
       "    (layers): Sequential(\n",
       "      (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.16000000000000003)\n",
       "      (2): Linear(in_features=1200, out_features=50, bias=True)\n",
       "      (3): ReLU(inplace)\n",
       "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.1)\n",
       "      (6): Linear(in_features=50, out_features=10, bias=True)\n",
       "    )\n",
       "  )\n",
       "), opt_func=functools.partial(<class 'torch.optim.adam.Adam'>, betas=(0.9, 0.99)), loss_func=FlattenedLoss of CrossEntropyLoss(), metrics=[<function accuracy at 0x2adf5c7b6d90>], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath('.'), model_dir='models', callback_fns=[functools.partial(<class 'fastai.basic_train.Recorder'>, add_time=True, silent=False)], callbacks=[RNNTrainer\n",
       "learn: ...\n",
       "alpha: 2.0\n",
       "beta: 1.0], layer_groups=[Sequential(\n",
       "  (0): Embedding(46, 400, padding_idx=1)\n",
       "  (1): EmbeddingDropout(\n",
       "    (emb): Embedding(46, 400, padding_idx=1)\n",
       "  )\n",
       "), Sequential(\n",
       "  (0): WeightDropout(\n",
       "    (module): LSTM(400, 1150, batch_first=True)\n",
       "  )\n",
       "  (1): RNNDropout()\n",
       "), Sequential(\n",
       "  (0): WeightDropout(\n",
       "    (module): LSTM(1150, 1150, batch_first=True)\n",
       "  )\n",
       "  (1): RNNDropout()\n",
       "), Sequential(\n",
       "  (0): WeightDropout(\n",
       "    (module): LSTM(1150, 400, batch_first=True)\n",
       "  )\n",
       "  (1): RNNDropout()\n",
       "), Sequential(\n",
       "  (0): PoolingLinearClassifier(\n",
       "    (layers): Sequential(\n",
       "      (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.16000000000000003)\n",
       "      (2): Linear(in_features=1200, out_features=50, bias=True)\n",
       "      (3): ReLU(inplace)\n",
       "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.1)\n",
       "      (6): Linear(in_features=50, out_features=10, bias=True)\n",
       "    )\n",
       "  )\n",
       ")], add_time=True, silent=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.load('single_cat_small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Category Burgers,\n",
       " tensor(0),\n",
       " tensor([0.4355, 0.0104, 0.0069, 0.2434, 0.0332, 0.0399, 0.0934, 0.0090, 0.1201,\n",
       "         0.0082]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.predict(\"mcdonalds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Category Fish & Chips,\n",
       " tensor(4),\n",
       " tensor([1.5580e-03, 3.1403e-04, 8.1530e-05, 4.2188e-04, 9.9319e-01, 1.8859e-04,\n",
       "         6.3263e-04, 8.6976e-05, 3.4305e-03, 9.9172e-05]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.predict(\"codfather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Category Indian,\n",
       " tensor(5),\n",
       " tensor([0.0337, 0.0355, 0.0686, 0.0039, 0.0087, 0.5477, 0.0711, 0.1509, 0.0623,\n",
       "         0.0176]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.predict(\"sapna\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Category Desserts,\n",
       " tensor(3),\n",
       " tensor([0.1098, 0.0801, 0.0437, 0.1995, 0.0397, 0.1736, 0.1095, 0.0361, 0.1297,\n",
       "         0.0783]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.predict(\"fire and dough\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Category Chinese,\n",
       " tensor(2),\n",
       " tensor([1.5544e-03, 1.2759e-02, 8.5258e-01, 5.9515e-04, 3.1316e-03, 2.2290e-02,\n",
       "         4.8611e-02, 9.7889e-03, 3.1811e-02, 1.6876e-02]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.predict(\"top chef\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Category Pizza,\n",
       " tensor(8),\n",
       " tensor([0.1552, 0.0066, 0.0363, 0.0036, 0.1184, 0.1063, 0.0786, 0.0043, 0.4622,\n",
       "         0.0286]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.predict(\"papa johns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Category Fish & Chips,\n",
       " tensor(4),\n",
       " tensor([0.0680, 0.0146, 0.0171, 0.0006, 0.6248, 0.0195, 0.1366, 0.0016, 0.1045,\n",
       "         0.0126]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.predict(\"gormans\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
